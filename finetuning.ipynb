{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning\n",
    "\n",
    "- Using pretrained model (MLM) weights as the base for the finetuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deciding on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check gpu(s)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick gpu\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Essentials (libraries, config etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJjuBps9hHmE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup, AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Sampler, Dataset, DataLoader\n",
    "from IPython.display import display\n",
    "from accelerate import Accelerator\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#import more_itertools\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4s_KxVLSg9-2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#configuration, change to fit your use case\n",
    "class cfg():\n",
    "    max_len = 100 #token max length\n",
    "    data_folder = \"/path/to/data/\"\n",
    "    model_name = \"TurkuNLP/bert-base-finnish-cased-v1\"\n",
    "    pt_model_path='/path/to/mlm_output_folder/pytorch_model.bin'\n",
    "    train_batch_size = 32\n",
    "    valid_batch_size = 64\n",
    "    test_batch_size = 64\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\" #in case no GPU is available, we run with CPU\n",
    "    debug = True\n",
    "    seed = 2022\n",
    "    epochs = 2\n",
    "    \n",
    "    n_folds = 2 \n",
    "    train_folds = [0, 1]\n",
    "    \n",
    "    num_labels = 3\n",
    "    labels = [0, 1, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOX5AQdBg9-1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set seeds\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7KDm7u3g9-2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load training and testing data as pandas dataframes\n",
    "train =  pd.read_csv(cfg.data_folder+'finetune_trainset.csv')\n",
    "train = train[['label','text']]\n",
    "test = pd.read_csv(cfg.data_folder+'finetune_testset.csv')\n",
    "test = test[['label','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3ZWYLNYg9-3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check if the model runs with fewer data samples\n",
    "if cfg.debug:\n",
    "    cfg.train_batch_size=4\n",
    "    cfg.valid_batch_size=8\n",
    "    train = train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDgx5T6cg9-3",
    "outputId": "3f5c3ec6-1c74-4673-8742-8056ec07f822",
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(train)\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3ukPUR5g9-4",
    "outputId": "9853a90b-7461-464d-988e-0fa9133db247",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#stratified kfold for creating training and validation datasets later on\n",
    "mskf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=2022)\n",
    "\n",
    "for fold, (trn_, val_) in enumerate(mskf.split(train, train[\"label\"])):\n",
    "    print(len(trn_), len(val_))\n",
    "    train.loc[val_, \"kfold\"] = fold\n",
    "    \n",
    "train[\"kfold\"] = train[\"kfold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDUrVeGGg9-4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df[\"text\"].values\n",
    "        self.is_train = False\n",
    "        if \"label\" in df.columns:\n",
    "            self.labels = df[\"label\"].values\n",
    "            self.is_train = True\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "              text, label = self.texts[idx], self.labels[idx]\n",
    "        else:\n",
    "              text = self.texts[idx]\n",
    "        \n",
    "        example = tokenizer(text, max_length=cfg.max_len, \n",
    "                                 padding=\"max_length\", add_special_tokens=True, truncation=True, return_attention_mask=True,\n",
    "        return_token_type_ids=True)\n",
    "        example[\"input_ids\"] = torch.tensor(example[\"input_ids\"])\n",
    "        example[\"token_type_ids\"] = torch.tensor(example[\"token_type_ids\"])\n",
    "        example[\"attention_mask\"] = torch.tensor(example[\"attention_mask\"])\n",
    "        if self.is_train:\n",
    "            return example, torch.tensor(label)\n",
    "        else:\n",
    "            return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U46TerTg9-5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/rhtsingh/utilizing-transformer-representations-efficiently\n",
    "class WeightedLayerPooling(torch.nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else torch.nn.Parameter(\n",
    "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZZMNvdTrgYu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        tconfig = AutoConfig.from_pretrained(cfg.model_name)\n",
    "        tconfig.update({'output_hidden_states': True})\n",
    "        tconfig.update({'num_labels': cfg.num_labels})\n",
    "        self.model = AutoModel.from_pretrained(cfg.pt_model_path, config=tconfig)\n",
    "        self.model.base_model.embeddings.requires_grad_(False)\n",
    "        self.fc = torch.nn.Linear(tconfig.hidden_size, cfg.num_labels)\n",
    "        self.pooler = WeightedLayerPooling(tconfig.num_hidden_layers, layer_start=9, layer_weights=None)\n",
    "        self.ms_dropout = [torch.nn.Dropout(x/10) for x in range(5)]\n",
    "        self.dp = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, inputs):        \n",
    "        out_e = self.model(**inputs)\n",
    "        out = torch.stack(out_e[\"hidden_states\"])\n",
    "        out = self.pooler(out)\n",
    "        for i, fc_dp in enumerate(self.ms_dropout):\n",
    "            if i == 0:\n",
    "                outputs = self.fc(fc_dp(out[:, 0]))\n",
    "            else:\n",
    "                outputs += self.fc(fc_dp(out[:, 0]))\n",
    "        outputs = self.fc(self.dp(out[:,0]))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Logging information during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMc2_zacg9-6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation metrics (accuracy, f1-score, precision, recall) calculated\n",
    "def get_eval_metrics(labels, preds, avg_type='weighted', print_metrics=True):\n",
    "    if isinstance(labels, list):\n",
    "        labels = torch.cat(labels).cpu()\n",
    "    if isinstance(preds, list):\n",
    "        preds = torch.cat(preds).cpu()\n",
    "    acc_score = accuracy_score(labels, preds)\n",
    "    ff1_score = f1_score(labels, preds, average=avg_type, labels=cfg.labels)\n",
    "    rec_score = recall_score(labels, preds, average=avg_type, labels=cfg.labels)\n",
    "    prec_score = precision_score(labels, preds, average=avg_type, labels=cfg.labels)\n",
    "    \n",
    "    if print_metrics:\n",
    "        print(f\"accuracy score: {acc_score}\")\n",
    "        print(f\"f1 score: {ff1_score}\")\n",
    "        print(f\"recall score: {rec_score}\")\n",
    "        print(f\"precision score: {prec_score}\")\n",
    "    \n",
    "    return [acc_score, ff1_score, rec_score, prec_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZKkk5SAg9-6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, optimizer, loss_fn, scheduler, epoch, fold, valid_dataloader):\n",
    "    model.train()\n",
    "    print(\"=\"*15, \">\" f\"Fold {fold+1} Epoch {epoch}\", \"<\", \"*\"*15, \"\\n\\n\")\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    for batch_idx, (example, labels) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k : v.to(cfg.device) for (k, v) in example.items()}\n",
    "        labels = torch.tensor(labels)\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            out = model(inputs).squeeze()\n",
    "        \n",
    "        loss = loss_fn(out.cpu().float(), labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        losses.update(loss.item(), cfg.train_batch_size)\n",
    "        \n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}] | Batch Number: [{batch_idx+1}/{len(dataloader)}] | Loss: [{losses.avg}]\\n\")\n",
    "            \n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xx2Nz5Jig9-6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_fn(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    metrics = AverageMeter()\n",
    "    val_accuracy = []\n",
    "    val_preds = []\n",
    "    val_f1 = []\n",
    "    val_precision=[]\n",
    "    val_recall=[]\n",
    "    val_labels=[]\n",
    "    for batch_idx, (example, labels) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        inputs = {k : v.to(cfg.device) for (k, v) in example.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model(inputs).squeeze()\n",
    "        loss = loss_fn(out.cpu(), labels.long())\n",
    "        losses.update(loss.item(), cfg.train_batch_size)\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(out.cpu(), dim=1).flatten()\n",
    "        val_preds.append(preds)\n",
    "        val_labels.append(labels)\n",
    "\n",
    "    return losses.avg, val_labels, val_preds   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgYL_j4Vg9-6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fold(fold): \n",
    "    train_df = train[train[\"kfold\"] != fold]\n",
    "    valid_df = train[train[\"kfold\"] == fold]\n",
    "    \n",
    "    global tokenizer \n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "    train_dataset = ClassificationDataset(train_df)\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, num_workers=2, batch_size=cfg.train_batch_size)\n",
    "    valid_dataset = ClassificationDataset(valid_df)\n",
    "    valid_dataloader = DataLoader(valid_dataset, shuffle=True, num_workers=2, batch_size=cfg.valid_batch_size)\n",
    "    \n",
    "    model = Model()\n",
    "    model.to(cfg.device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\": model.fc.parameters(), \"lr\": 3e-5},\n",
    "        {\"params\": model.pooler.parameters(), \"lr\": 3e-5},\n",
    "        {\"params\": model.model.parameters(), \"lr\": 1e-5},\n",
    "    ],\n",
    "    lr=5e-4)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=0, \n",
    "                                                num_cycles=0.5, \n",
    "                                                num_training_steps=int(len(train_dataset) / cfg.train_batch_size * cfg.epochs))\n",
    "\n",
    "    \n",
    "    #best_val_loss=np.inf #for saving fewer checkpoints\n",
    "    for epoch in range(cfg.epochs):\n",
    "        #training\n",
    "        train_loss = train_epoch(train_dataloader, model, optimizer, nn.CrossEntropyLoss(), scheduler, epoch+1, fold,\n",
    "                                 valid_dataloader)\n",
    "\n",
    "        #validation\n",
    "        valid_loss, valid_labels, valid_pred  = validate_fn(valid_dataloader, model, nn.CrossEntropyLoss())\n",
    "        print(\"=\"*15, \">\" f\"Fold {fold+1} Epoch {epoch+1} Results:\", \"<\", \"*\"*15, \"\\n\\n\")\n",
    "        print(f\"Training Loss: {train_loss}\")\n",
    "        print(f\"Validation Loss: {valid_loss}\")\n",
    "        _ = get_eval_metrics(valid_labels, valid_pred)\n",
    "        \n",
    "        #saving model\n",
    "        #if valid_loss < best_val_loss: #for saving fewer checkpoints\n",
    "        print(\"SAVING MODEL: {} fold, {} epoch, valid_loss: {: >4.5f}\".format(fold+1,epoch+1, valid_loss))\n",
    "        #best_val_loss = valid_loss #for saving fewer checkpoints\n",
    "        torch.save(model.state_dict(), f\"finbert_base_fold_{fold+1}_epoch_{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Training..\")\n",
    "for fold in cfg.train_folds:\n",
    "    train_fold(fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    val_probs = []\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    for batch_idx, (example, labels) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        inputs = {k : v.to(cfg.device) for (k, v) in example.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model(inputs).squeeze()\n",
    "        loss = loss_fn(out.cpu(), labels.long())\n",
    "        losses.update(loss.item(), cfg.train_batch_size)\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(out.cpu(), dim=1).flatten()\n",
    "        probs = out\n",
    "        val_probs.append(probs)\n",
    "        val_preds.append(preds)\n",
    "        val_labels.append(labels)\n",
    "               \n",
    "    return val_labels, val_preds, val_probs, losses.avg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dataset = ClassificationDataset(test)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, num_workers=2, batch_size=cfg.test_batch_size)\n",
    "preds_in_all_folds_val=[]\n",
    "probs_in_all_folds_val=[]\n",
    "\n",
    "for fold_num in range(cfg.n_folds):\n",
    "    pth = f\"finbert_base_fold_{fold_num+1}_epoch_2.pth\"\n",
    "    model = Model().to(cfg.device)\n",
    "    model.load_state_dict(torch.load(pth))\n",
    "    \n",
    "    labels, preds, probs1, test_loss = test_fn(test_dataloader, model, nn.CrossEntropyLoss())\n",
    "    preds_in_all_folds_val.append(preds)\n",
    "    probs_in_all_folds_val.append(probs1)\n",
    "    print(\"Testing fold \",fold_num+1)\n",
    "    print(f\"Test loss: {test_loss}\")\n",
    "    _ = get_eval_metrics(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor sum \n",
    "def sum_of_tensors(probs_in_all_folds_val):\n",
    "    probs_sum = torch.cat(probs_in_all_folds_val[0]).cpu()\n",
    "    for i in range(1,len(probs_in_all_folds_val)):\n",
    "        probs_sum = probs_sum + torch.cat(probs_in_all_folds_val[i]).cpu()\n",
    "    return probs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_all_folds = torch.argmax((sum_of_tensors(probs_in_all_folds_val)/len(probs_in_all_folds_val)), dim=1)\n",
    "print(\"Testing..\")\n",
    "_ = get_eval_metrics(labels, preds_all_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
